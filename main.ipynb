{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from nnfs.datasets import spiral_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataset with 100 data points for 3 classes of spirals\n",
    "#Results in 300 total data points for classification\n",
    "X, y = spiral_data(100, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating hidden layers\n",
    "class Layer_Dense:\n",
    "    #Initialize using the number of inputs and neurons\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        #Create an array of size (n_inputs x n_neurons) that are random based on the normal distrution\n",
    "        #scaled by a tenth\n",
    "        self.w = 0.1 * np.random.randn(n_inputs, n_neurons)\n",
    "        #Create a row vector based on the number of neurons\n",
    "        self.bias = np.zeros([1, n_neurons])\n",
    "    #Method to compute the output, takes in an input matrix\n",
    "    def forward(self, inputs):\n",
    "        self.output = inputs@self.w + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementation of ReLU function        \n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "class Activation_Softmax:\n",
    "    def forward(self, input):\n",
    "        exp_val = np.exp(input - np.max(input, axis=1, keepdims=True))\n",
    "        probability = exp_val/np.sum(exp_val, axis=1, keepdims=True)\n",
    "        \n",
    "        self.output = probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Rectified Linear Activation function is a piecewise function that  can be demonstrated as the following:\n",
    "\n",
    "\\begin{align*}\n",
    "    &\\text{If } x>0,     &  x\\\\\n",
    "    &\\text{If } x\\leq 0, &  0\n",
    "\\end{align*}\n",
    "\n",
    "This is important because it is a fast compututation that allows for the fitting of a non-linear signal given several neurons. This achieved by using a combination of the soft clipping for negative values and linear maping for natural number.\n",
    "\n",
    "The second activation function is to create normalization through the Softmax function. This can be displayed as the following:\n",
    "\n",
    "\\begin{equation*}\n",
    "    S_{i,j} = \\frac{e^{z_{i,j}}}{\\sum_{l=1}^{L}e^{z_{i,j},l}}\n",
    "\\end{equation*}\n",
    "\n",
    "This computes a quotient of the elements of the matrix z as powers of e and the sum of each of the elements of the same matrix z as the powers of e."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1st Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the first hidden layer with 2 input and 3 output features\n",
    "layer1 = Layer_Dense(2,3)\n",
    "activation1 = Activation_ReLU()\n",
    "layer1.forward(X)\n",
    "activation1.forward(layer1.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is creating 3 neurons that take in 2 inputs. This layer is layer is forward propogated with inputs $x$, weights $w$, and bias $b$ for each neuron in the model. The relationship of the output $o_1$ can shown as the following:\n",
    "\n",
    "\\begin{equation*}\n",
    "    o_1  =  x^{T} w + b\n",
    "\\end{equation*}\n",
    "\n",
    "The activation of a neuron is obtained by using the ReLU activation function, described above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2nd Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create second hidden layer with 3 input and 3 output features\n",
    "layer2 = Layer_Dense(3,3)\n",
    "activation2 = Activation_Softmax()\n",
    "layer2.forward(activation1.output)\n",
    "activation2.forward(layer2.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This second hidden layer is created with 3 neurons taking three inputs and results in three outputs. The input of this layer is found by taking the output of the first hidden layer, $o_1$. The forward propogation with weights $w$, bias $b$, and output $o_2$ can be shown as the following:\n",
    "\n",
    "\\begin{equation*}\n",
    "    o_2 = o_1^T w + b\n",
    "\\end{equation*}\n",
    "\n",
    "This is the result of our last layer. Unlike the previous the usage of the softmax activation function is used to modify the data, so the output is between zero and one, $0 \\leq o_1 \\leq 1$. And the sum of the outputs is equal to one, $\\sum_{i=1}^n o_1 = 1$.\n",
    "\n",
    "This is important in the last layer because it allows for a probability associated with the activation of neurons in the last layer. This is beneficial compared to the ReLU activation function because it does not lose data. The ReLU function can be detrimental in this stage for a classification model because it clips the negati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 3)\n"
     ]
    }
   ],
   "source": [
    "print(activation2.output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
