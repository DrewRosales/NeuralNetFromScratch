{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from nnfs.datasets import spiral_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataset with 100 data points (batch of 100 points) for 3 classes of spirals\n",
    "#Results in 300 total data points for classification\n",
    "X, y = spiral_data(100, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating hidden layers\n",
    "class Layer_Dense:\n",
    "    #Initialize using the number of inputs and neurons\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        #Create an array of size (n_inputs x n_neurons) that are random based on the normal distrution\n",
    "        #scaled by a tenth\n",
    "        self.w = 0.1 * np.random.randn(n_inputs, n_neurons)\n",
    "        #Create a row vector based on the number of neurons\n",
    "        self.bias = np.zeros([1, n_neurons])\n",
    "    #Method to compute the output, takes in an input matrix\n",
    "    def forward(self, inputs):\n",
    "        self.output = inputs@self.w + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementation of ReLU function        \n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "class Activation_Softmax:\n",
    "    def forward(self, input):\n",
    "        exp_val = np.exp(input - np.max(input, axis=1, keepdims=True))\n",
    "        probability = exp_val/np.sum(exp_val, axis=1, keepdims=True)\n",
    "        \n",
    "        self.output = probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Rectified Linear Activation function is a piecewise function that  can be demonstrated as the following:\n",
    "\n",
    "\\begin{align*}\n",
    "    &\\text{If } x>0,     &  x\\\\\n",
    "    &\\text{If } x\\leq 0, &  0\n",
    "\\end{align*}\n",
    "\n",
    "This is important because it is a fast compututation that allows for the fitting of a non-linear signal given several neurons. This achieved by using a combination of the soft clipping for negative values and linear mapping for natural numbers.\n",
    "\n",
    "The second activation function is to create normalization through the Softmax function. This can be displayed as the following:\n",
    "\n",
    "\\begin{equation*}\n",
    "    S_{i,j} = \\frac{e^{z_{i,j}}}{\\sum_{l=1}^{L}e^{z_{i,j},l}}\n",
    "\\end{equation*}\n",
    "\n",
    "This computes a quotient of the elements of the matrix z as powers of e and the sum of each of the elements of the same matrix z as the powers of e."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cost:\n",
    "    #Calculates the Categorical Cross-Entropy Loss\n",
    "    #using the output of the model and the given labels, y.\n",
    "    def calculate(self, output, y):\n",
    "        sample_loss = self.forward(output, y)\n",
    "        \n",
    "        data_loss = np.mean(sample_loss)\n",
    "        \n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Cross Entropy Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cost_CatergoricalCrossEntropy(Cost):\n",
    "    def forward(self, y_pred, y_exact):\n",
    "        \n",
    "        y_pred_clip = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        #The case when the labels are given as a vector rather than a matrix, 1D\n",
    "        if len(y_exact.shape) == 1:\n",
    "            confidence = y_pred_clip[range(samples), y_exact]\n",
    "        \n",
    "        #The labels are given in terms of an matrix,\n",
    "        #giving the confidence that corresponding to the labels that solve\n",
    "        #the classification\n",
    "        elif len(y_exact.shape) == 2:\n",
    "            confidence = np.sum(y_pred_clip*y_exact, axis=1)\n",
    "            \n",
    "        #Loss\n",
    "        negative_log = -np.log(confidence)\n",
    "        return negative_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Categorical Cross-Entropy function is an error function that is computed by taking the log of the output from the model then multiplying it with the corresponding label, either zero or one, and negating the sum of all these entries:\n",
    "\n",
    "\\begin{equation*}\n",
    "L_i = -\\sum_j y_{i,j}log(\\hat{y}_{i,j})\n",
    "\\end{equation*}\n",
    "\n",
    "The labels are given as a matrix, $y_{i,j}$, and the expected values or the labels are given as $\\hat{y}_{i,j}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1st Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the first hidden layer with 2 input and 3 output features\n",
    "layer1 = Layer_Dense(2,3)\n",
    "activation1 = Activation_ReLU()\n",
    "layer1.forward(X)\n",
    "activation1.forward(layer1.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is creating 3 neurons that take in 2 inputs. This layer is layer is forward propogated with inputs $x$, weights $w$, and bias $b$ for each neuron in the model. The relationship of the output $o_1$ can shown as the following:\n",
    "\n",
    "\\begin{equation*}\n",
    "    o_1  =  x^{T} w + b\n",
    "\\end{equation*}\n",
    "\n",
    "The activation of a neuron is obtained by using the ReLU activation function, described above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2nd Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create second hidden layer with 3 input and 3 output features\n",
    "layer2 = Layer_Dense(3,3)\n",
    "activation2 = Activation_Softmax()\n",
    "layer2.forward(activation1.output)\n",
    "activation2.forward(layer2.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This second hidden layer is created with 3 neurons taking three inputs and results in three outputs. The input of this layer is found by taking the output of the first hidden layer, $o_1$. The forward propogation with weights $w$, bias $b$, and output $o_2$ can be shown as the following:\n",
    "\n",
    "\\begin{equation*}\n",
    "    o_2 = o_1^T w + b\n",
    "\\end{equation*}\n",
    "\n",
    "This is the result of our last layer. Unlike the previous the usage of the softmax activation function is used to modify the data, so the output is between zero and one, $0 \\leq o_1 \\leq 1$. And the sum of the outputs is equal to one, $\\sum_{i=1}^n o_1 = 1$.\n",
    "\n",
    "This is important in the last layer because it allows for a probability associated with the activation of neurons in the last layer. This is beneficial compared to the ReLU activation function because it does not lose data. The ReLU function can be detrimental in this stage for a classification model because it clips the negative data to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "Cost: 1.0989235380413012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drros\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "#Create cost_function as a member of the Cost_CategoricalCrossEntropy class\n",
    "cost_function = Cost_CatergoricalCrossEntropy()\n",
    "cost, accuracy = cost_function.calculate(activation2.output, y)\n",
    "print(\"Cost:\", cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstration of a fairly large Loss since the the configuration of the model is random and there is not backpropogation implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
